{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "753877c5",
   "metadata": {},
   "source": [
    "### 비용 최소화 하기 - 경사 하강법(Gradient Descent) 소개\n",
    "\n",
    "* W 파라미터 개수가 적다면 고차원 방정식으로 비용 함수가 최소가 되는 W 변숫값을 도출할 수 있겠지만, W 파라미터가 많으면 고차원 방정식을 동원하더라도 해결하기가 어렵습니다. 경사 하강법은 이러한 고차원 방정식에 대한 문제를 해결해 주면서 비용 함수 RSS를 최소화하는 방법을 직관적으로 제공하는 뛰어난 방식입니다.\n",
    "\n",
    "* **많은 W 파라미터가 있는 경우에 경사 하강법은 보다 간단하고 직관적인 비용함수 최소화 솔루션을 제공**\n",
    "* 경사 하강법의 사전적 의미인 '점진적인 하강'이라는 뜻에서도 알수 있듯이, **'점진적으로' 반복적인 계산을 통해 W 파라미터값을 업데이트하면서 오류 값이 최소가 되는 W 파라미터를 구하는 방식입니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d90dc62",
   "metadata": {},
   "source": [
    "* 경사 하강법은 반복적으로 비용 함수의 반환 값, 즉 예측값과 실제 값의 차이가 작아지는 방향성을 가지고 W 파라미터를 지속해서 보정해 나갑니다.\n",
    "* 최초 오류 값이 100이었다면 두번쨰 오류값은 90, 세번쨰는 80과 같은 방식으로 지속해서 오류를 감소시키는 방향으로 W 값을 계속 업데이트해 나갑니다.\n",
    "* 그리고 오류 갑싱 더 이상 작아지지 않으면, 그 오류 값을 최소 비용으로 판단하고 그 떄의 W 값을 최적 파라미터로 반환합니다.\n",
    "\n",
    "* **경사 하강법의 핵심은 \"어떻게 하면 오류가 작아지는 방향으로 W값을 보정할 수 있을까?\"입니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314cae7c",
   "metadata": {},
   "source": [
    "### 손실 함수의 편미분\n",
    "\n",
    "* Loss(w)는 MSE이고 Loss(w) = $\\frac{1}{N}\\sum^N_{i=1}{(y_i - (w_0+w_1*x_i))^2}$입니다.\n",
    "* MES Loss 함수는 변수가 w 파라미터로 이뤄진 함수입니다.\n",
    "* Loss(w)를 미분해 미분 함수의 최솟값을 구해야 하는데, Loss(w)는 두 개의 파라미터인 w0과 w1를 각각 가지고 있기 떄문에 일반적인 미분을 적용할 수가 없고, w0, w1 각 변수에 편미분을 적용해야 합니다.\n",
    "* Loss(w)를 최소화하는 w0과 w1의 값은 각각 r(w)를 w0, w1으로 순차적으로 편미분을 수행해 얻을 수 있습니다.\n",
    "\n",
    "* $ \\frac{dLoss(w)}{dw_1} = \\frac{2}{N}\\sum^{N}_{i=1}-x_i*(y_i-(w_0+w_1x_i) = -\\frac{2}{N}\\sum^{N}_{i=1}x_i*(실제값_i - 예측값_i) $\n",
    "* $ \\frac{dLoss(w)}{dw_0} = \\frac{2}{N}\\sum^{N}_{i=1}-(y_i-(w_0+w_1x_i) = -\\frac{2}{N}\\sum^{N}_{i=1}(실제값_i - 예측값_i) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec56304",
   "metadata": {},
   "source": [
    "### 가중치(Weight), 절편(Bias) 업데이트 하기\n",
    "\n",
    "* 가중치/절편 W 값은 손실 함수의 편미분 값을 Update하면서 계속 갱신함\n",
    "* Update는 기존 W값에 손실 함수 편미분 값을 감소시키는 방식을 적용하되 편미분 값을 그냥 감소시키지 않고 일정한 계수를 곱해서 감소시키며, 이를 학습률이라고 함.\n",
    "\n",
    "* $\\eta = 학습률(learning rate)$\n",
    "* $w_{1,new} = w_{1,old} - \\eta\\frac{dLoss(w)}{dw_1} = w_{1,old}+\\eta(\\frac{2}{N}\\sum^{N}_{i=1}x_i*(실제값_i - 예측값_i))$\n",
    "* $w_{0,new} = w_{0,old} - \\eta\\frac{dLoss(w)}{dw_0} = w_{0,old}+\\eta(\\frac{2}{N}\\sum^{N}_{i=1}실제값_i - 예측값_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11afd930",
   "metadata": {},
   "source": [
    "### 퍼셉트론 기반 단순 선형 회귀에서 Gradient Descent 적용하기\n",
    "\n",
    "* 정해진 iteration 수만큼 손실함수 편미분을 구하고 Weight, Bias update 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f032a",
   "metadata": {},
   "source": [
    "* for i in range(iter_epochs):\n",
    "    1. 손실함수를 기반으로 Weight와 Bias 편미분 값을 구함\n",
    "    2. 구해진 편미분 값을 기존의 Weight와 Bias에 Update 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c19a90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
