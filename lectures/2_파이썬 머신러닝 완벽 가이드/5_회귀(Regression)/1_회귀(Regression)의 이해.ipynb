{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830b56d7",
   "metadata": {},
   "source": [
    "### 회귀(Regression)\n",
    "\n",
    "<br>\n",
    "\n",
    "* **여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법을 통칭합니다.**\n",
    "<br>\n",
    "\n",
    "* ex) 아파트 가격은? <= 방 개수, 아파트 크기, 주변 학군, 근처 지하철 역 개수\n",
    "    * Y = W1X1 + W2X2 + W3X3 + WnXn\n",
    "        * Y는 종속변수, 즉 아파트 가격\n",
    "        * X1, X2, X3, Xn은 방 개수, 아파트 크기, 주변 학군 등의 독립 변수\n",
    "        * W1, W2, W3, Wn은 이 독립변수의 값에 영향을 미치는 회귀 계수(Regression coefficients)\n",
    "<br><br>\n",
    "* **머신러닝 회귀 예측의 핵심은 주어진 피처와 결정 값 데이터 기반에서 학습을 통해 최적의 회귀 계수를 찾아내는 것입니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744d9c54",
   "metadata": {},
   "source": [
    "### 회귀의 유형\n",
    "\n",
    "* 선형/비선형 여부\n",
    "    * 회귀 계수가 선형인지 아닌지\n",
    "* 독립변수의 개수\n",
    "    * 독립변수의 개수가 1개인지 아닌지(단일 회귀, 다중 회귀)\n",
    "* 종속변수의 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33cda0c",
   "metadata": {},
   "source": [
    "### 분류(Classification)와 회귀(Regression)\n",
    "\n",
    "* Classification -> Category값(이산값)\n",
    "* Regression -> 숫자값(연속값)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cdf01a",
   "metadata": {},
   "source": [
    "### 선형 회귀의 종류\n",
    "\n",
    "* 일반 선형 회귀 : 예측값과 실제값의 RSS(Residual Sum of Squares)를 최소화할 수 있도록 회귀 계수를 최적화하며, 규제(Regularization)을 적용하지 않은 모델\n",
    "* 릿지(Ridge) : 릿지 회귀는 선형 회귀에 L2 규제를 적용한 회귀 모델\n",
    "* 라쏘(Lasso) : 라쏘 회귀는 선형 회귀에 L1 규제를 적용한 회귀 모델\n",
    "* 엘라스틱넷(ElasticNet) : L2, L1 규제를 함께 적용한 회귀 모델\n",
    "* 로지스틱 회귀(Logistic Regression) : 로지스틱 회귀는 회귀라는 이름이 붙어 있지만, 사실은 분류에 사용되는 선형 모델입니다.\n",
    "    * 이산값을 예측, 이진 분류에 효과적인 회귀 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a136781",
   "metadata": {},
   "source": [
    "### 단순 선형 회귀(Simple Regression)를 통한 회귀의 이해\n",
    "\n",
    "* 주택 가격이 주택의 크기로만 결정되는 단순 선형 회귀로 가정하면 다음과 같이 주택 가격은 주택 크기에 대해 선형(직선 형태)의 관계로 표현할 수 있습니다.\n",
    "* 최적의 회귀 모델을 만든다는 것은 바로 전체 데이터의 잔차(오류값) 합이 최소가 되는 모델을 만든다는 의미이며, 동시에 오류 값 합이 최소가 될 수 있는 최적의 회귀 계수를 찾는다는 의미도 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ff327",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d86fd10",
   "metadata": {},
   "source": [
    "### RSS 기반의 회귀 오류 측정\n",
    "\n",
    "* RSS : 오류 값의 제곱을 구해서 더하는 방식입니다. 일반적으로 미분 등의 계산을 편리하게 하기 위해서 RSS 방식으로 오류 합을 구합니다. \n",
    "    * 즉 Error^2 = RSS입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a29d7b",
   "metadata": {},
   "source": [
    "### RSS의 이해\n",
    "\n",
    "* RSS는 이제 변수가 W0, W1인 식으로 표현할 수 있으며, 이 RSS를 최소로 하는 W0, W1, 즉 회귀 계수를 학습을 통해서 찾는 것이 머신러닝 기반 회귀의 핵심 사항입니다.\n",
    "* RSS는 회귀식의 독립변수 X, 종속변수 Y가 중심변수가 아니라 w변수(회귀 계수)가 중심 변수임을 인지하는 것이 매우 중요합니다.\n",
    "    * 학습 데이터로 입력되는 독립변수와 종속변수는 RSS에서 모두 상수로 간주합니다.\n",
    "* 일반적으로 RSS는 학습 데이터의 건수로 나누어서 다음과 같이 정규화된 식으로 표현됩니다.\n",
    "    * $RSS(w_0, w_1) = \\frac{1}{N}\\sum_{i=1}^{N} (y_i - (w_0 + w_1 * x_i))^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb43126d",
   "metadata": {},
   "source": [
    "### RSS - 회귀의 비용 함수\n",
    "<br>\n",
    "\n",
    "$RSS(w_0, w_1) = \\frac{1}{N}\\sum_{i=1}^{N} (y_i - (w_0 + w_1 * x_i))^2 $\n",
    "\n",
    "* 회귀에서 이 RSS는 비용(Cost)이며 w변수(회귀 계수)로 구성되는 RSS를 비용 함수라고 합니다.\n",
    "* 머신 러닝 회귀 알고리즘은 데이터를 계속 학습하면서 이 비용 함수가 반환하는 값(즉, 오류 값)을 지속해서 감소시키고 최종적으로는 더 이상 감소하지 않는 최소의 오류 값을 구하는 것입니다.\n",
    "* 비용 함수를 손실 함수(loss function)라고도 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4298320c",
   "metadata": {},
   "source": [
    "### 비용 최소화 하기 - 경사 하강법(Gradient Descent)\n",
    "\n",
    "* w 파라미터 개수가 적다면 고차원 방정식으로 비용 함수가 최소가 되는 w 변수값을 도출할 수 있겠지만, w 파라미터 개수가 많다면 고차원 방정식을 동원하더라도 해결하기가 어렵습니다.\n",
    "* 경사 하강법은 이러한 고차원 방정식에 대한 문제를 해결해주면서 비용 함수 RSS를 최소화하는 방법을 직관적으로 제공하는 뛰어난 방식입니다.\n",
    "* 경사 하강법의 사전적 의미인 '점진적인 하강'이라는 뜻에서도 알수 있듯이, '점진적으로' 반복적인 계산을 통해 w 파라미터값을 업데이트하면서 오류 값이 최소가 되는 w 파라미터를 구하는 방식입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a0862",
   "metadata": {},
   "source": [
    "### 경사 하강법 정리\n",
    "\n",
    "* w1, w0의 편미분 결과값인 $-\\frac{2}{N}\\sum_{i=1}^{N} x_i * (실제값_i * 예측값_i)$와 $-\\frac{2}{N}\\sum_{i=1}^{N} (실제값_i * 예측값_i)$을 반복적으로 보정하면서 w1, w0 값을 업데이트하면 비용합수 R(W)가 최소가 되는 w1, w0값을 구할 수 있습니다.\n",
    "* 하지만 실제로는 위 편미분 값이 너무 클 수 있기 때문에 보정계수 n을 곱하는데, 이를 \"학습률\"이라고 합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
