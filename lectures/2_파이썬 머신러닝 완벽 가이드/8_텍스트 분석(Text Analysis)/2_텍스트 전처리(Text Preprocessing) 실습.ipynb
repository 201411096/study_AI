{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b651cf18",
   "metadata": {},
   "source": [
    "## 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00fa1d9",
   "metadata": {},
   "source": [
    "### Text Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d593bd2a",
   "metadata": {},
   "source": [
    "### 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92949121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aaaaaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## conda install -c anaconda nltk\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75305953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "text_sample = \"\"\"The Matrix is everywhere its all around us, here even in this room.\n",
    "You can see it out your window or on your television.\n",
    "You feel it when you go to work, or go to church or pay your taxes.\"\"\"\n",
    "\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95787a2",
   "metadata": {},
   "source": [
    "### 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2577387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624c5f1",
   "metadata": {},
   "source": [
    "### 여러 문장들에 대한 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a92fde54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# 여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# 여러 문장들에 대해 문장별 단어 토큰화 수행.\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf9c816",
   "metadata": {},
   "source": [
    "### n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b842ba2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'Matrix', 'is'), ('Matrix', 'is', 'everywhere'), ('is', 'everywhere', 'its'), ('everywhere', 'its', 'all'), ('its', 'all', 'around'), ('all', 'around', 'us'), ('around', 'us', ','), ('us', ',', 'here'), (',', 'here', 'even'), ('here', 'even', 'in'), ('even', 'in', 'this'), ('in', 'this', 'room'), ('this', 'room', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "word = word_tokenize(sentence)\n",
    "\n",
    "all_ngrams = ngrams(words, 3)\n",
    "ngrams = [ngram for ngram in all_ngrams]\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc8d95f",
   "metadata": {},
   "source": [
    "### Stopwords 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52b51ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aaaaaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aaaaaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0241f2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수 :  179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop words 개수 : ', len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c6d57bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "# 위 예제의 3개의 문장별로 얻은 word_tokens list에 대해 stop word 제거 Loop\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거 Loop\n",
    "    for word in sentence:\n",
    "        # 소문자로 모두 변환합니다.\n",
    "        word = word.lower()\n",
    "        # tokenize된 개별 word가 stop words들의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64b06f",
   "metadata": {},
   "source": [
    "### Stemming과 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dc39812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49f4f793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
    "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
    "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146221f",
   "metadata": {},
   "source": [
    "### 텍스트의 피처 벡터화 유형\n",
    "\n",
    "1. BOW(Bag of words)\n",
    "    * Document Term Matrix : 개별 문서(또는 문장)을 단어들의 횟수나 정규화 변환된 횟수로 표현\n",
    "2. Word Embedding(Word2Vec)\n",
    "    * 개별 단어를 문맥을 가지는 N차원 공간에 벡터로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118956b9",
   "metadata": {},
   "source": [
    "### Bag of Words - BOW\n",
    "\n",
    "* Bag of Words 모델은 문서가 가지는 모든 단어(Words)를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델입니다. 문서 내 모든 단어를 한꺼번에 봉투(Bag)안에 넣은 뒤에 흔들어서 섞는다는 의미로 Bag of Words(BOW) 모델이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0517eb1",
   "metadata": {},
   "source": [
    "### Bag of Words 구조\n",
    "\n",
    "* 문장 1: My wife likes to watch baseball games and my daughter likes to watch baseball games too\n",
    "* 문장 2: My wife likes to play baseball\n",
    "\n",
    "1. 문장 1과 문장 2에 있는 모든 단어에서 중복을 제거하고 각 단어(feature 또는 term)를 컬럼 형태로 나열합니다. 그러고 나서 각 단어에 고유의 인덱스를 다음과 같이 부여합니다. 'and':0, 'baseball':1, 'daughter':2 ...\n",
    "2. 문장에서 해당 단어가 나타나는 횟수(Occurrence)를 각 단어(단어 인덱스)에 기재합니다. 예를 들어 baseball은 문장 1,2에서 총 2번 나타나며, daughter는 문장1에서만 1번 나타납니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ec2759",
   "metadata": {},
   "source": [
    "### BOW 장단점\n",
    "\n",
    "* 장점\n",
    "    * 쉽고 빠른 구축\n",
    "    * \"예상보다\" 문서의 특징을 잘 나타내어 전통적으로 여러분야에서 활용도가 높음\n",
    "* 단점\n",
    "    * 문맥 의미(semantic context) 반영 문제\n",
    "    * 희소 행렬 문제\n",
    "        * document term matrix가 가지는 고질적인 문제(값이 없는...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a4b45d",
   "metadata": {},
   "source": [
    "### BOW 피처 벡터화(Feature Vectorization)\n",
    "\n",
    "* M개의 Text문서들 -> M * N 피처 벡터화(m개의 문서, n개의 피처)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75abd7c1",
   "metadata": {},
   "source": [
    "### BOW 피처 벡터화 유형\n",
    "\n",
    "* 단순 카운트 기반의 벡터화\n",
    "    * 단어 피처에 값을 부여할 때, 각 문서에서 해당 단어가 나타나는 횟수, 즉 Count를 부여하는 경우를 카운트 벡터화라고 합니다. 카운트 벡터화에서는 카운트 값이 높을수록 중요한 단어로 인식됩니다.\n",
    "* TF-IDF 벡터화\n",
    "    * 카운트만 부여할 경우, 그 문서의 특징을 나타내기보다는 언어의 특성상 문장에서 자주 사용될 수밖에 없는 단어까지 높은 값을 부여하게 됩니다. 이러한 문제를 보완하기 위해 TF-IDF(Term Frequency Inverse Document Frequency) 벡터화를 사용합니다.\n",
    "    * TF-IDF는 **개별 문서에서 자주 나타나는 단어에 높은 가중치**를 주되, **모든 문서에서 전반적으로 자주 나타나는 단어에 대해서는 패널티**를 주는 방식으로 값을 부여합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d5a339",
   "metadata": {},
   "source": [
    "### TF-IDF(Term Frequency Inverse Document Frequency)\n",
    "\n",
    "* 특정 단어가 다른 문서에서 나타나지 않고, 특정 문서에서만 자주 사용된다면, 해당 단어는 해당 문서를 잘 특징짓는 중요 단어일 가능성이 높음\n",
    "* 특정 단어가 매우 많은 여러 문서에서 빈번히 나타난다면 해당 단어는 개별 문서를 특정짓는 정보로서의 의미를 상실\n",
    "\n",
    "1. TF(Term Frequency)\n",
    "    * 문서에서 해당 단어가 얼마나 나왔는지를 나타내는 지표\n",
    "2. DF(Document Frequency)\n",
    "    * 해당 단어가 몇개의 문서에서 나타났는지를 나타내는 좌표\n",
    "3. IDF(Inverse Docuent Frequency)\n",
    "    * DF의 역수로서 전체 문서수/DF\n",
    "\n",
    "* $TFIDF_i = TF_i * log \\frac{N}{DF_i}$\n",
    "    * $TF_i$ = 개별 단어에서의 단어 i의 빈도\n",
    "    * $DF_i$ = 단어 i를 가지고 있는 문서 개수\n",
    "    * N = 전체 문서 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56438452",
   "metadata": {},
   "source": [
    "### 사이킷런 CountVectorizer 초기화 파라미터\n",
    "* max_df\n",
    "    * 전체 문서에 걸쳐서 너무 높은 빈도수를 가지는 단어 피처를 제외하기 위한 파라미터입니다.\n",
    "    * max_df = 100 -> 100개 이하로 나타나는 단어만 피처로 추출\n",
    "    * max_df = 0.95 -> 0~95%까지의 단어만 피처로 추출\n",
    "* min_df\n",
    "    * 전체 문서에 걸쳐서 너무 낮은 빈도수를 가지는 단어 피처를 제외하기 위한 파라미터입니다.\n",
    "* max_features\n",
    "* stop_words\n",
    "    * stop_words = 'english'로 지정하면 영어의 스톱 워드로 지정된 단어는 추출에서 제외됩니다.\n",
    "* ngram_range\n",
    "    * default (1, 1) -> 단어를 하나씩..\n",
    "    * (1, 2) -> 단어를 하나씩 + 단어를 두개씩\n",
    "    * (2, 2) -> bigram만 ..\n",
    "* analyzer\n",
    "    * 피처 추출을 수행할 단위를 지정합니다. 디폴트는 'word'입니다.\n",
    "    * word가 아니라 character의 특정 범위를 피처로 만드는 특정한 경우 등을 적용할 때 사용됩니다.\n",
    "* token_pattern\n",
    "    * 토큰화를 수행하는 정규 표현식 패턴을 지정합니다.\n",
    "    * analyzer가 word인 경우에만 사용 가능하나, 디폴트 값을 변경할 경우는 거의 발생하지 않습니다.\n",
    "    * 어근 추출 시 외부 함수를 사용할 경우 해당 외부 함수를 token_pattern의 인자로 사용합니다.\n",
    "* lower_case\n",
    "    * 모든 문자를 소문자로 변경할 것인지를 설정, 기본은 True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f27ca",
   "metadata": {},
   "source": [
    "## Bag of Words - BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa809dac",
   "metadata": {},
   "source": [
    "### 사이킷런 CountVectorizer 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "305fcfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everywhere its all around us, here even in this room.                   You can see it out your window or on your television.                   You feel it when you go to work, or go to church or pay your taxes.', 'You take the blue pill and the story ends.  You wake in your bed and you believe whatever you want to believe                  You take the red pill and you stay in Wonderland and I show you how deep the rabbit-hole goes.'] \n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "text_sample_01 = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "                  You can see it out your window or on your television. \\\n",
    "                  You feel it when you go to work, or go to church or pay your taxes.'\n",
    "text_sample_02 = 'You take the blue pill and the story ends.  You wake in your bed and you believe whatever you want to believe\\\n",
    "                  You take the red pill and you stay in Wonderland and I show you how deep the rabbit-hole goes.'\n",
    "\n",
    "text = []\n",
    "\n",
    "text.append(text_sample_01)\n",
    "text.append(text_sample_02)\n",
    "\n",
    "print(text, \"\\n\", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ea2995",
   "metadata": {},
   "source": [
    "#### CountVectorizer 객체 생성 후 fit(), transform()으로 텍스트에 대한 feature vectorization 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c4dbb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Count Vectorization으로 feature extraction 변환 수행.\n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f04293aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr_vect = cnt_vect.transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6679ecaf",
   "metadata": {},
   "source": [
    "#### 피처 벡터화 후 데이터 유형 및 여러 속성 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc873f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 51)\n",
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t2\n",
      "  (0, 15)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t2\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 24)\t3\n",
      "  (0, 25)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 37)\t1\n",
      "  (0, 38)\t1\n",
      "  (0, 39)\t1\n",
      "  (0, 40)\t2\n",
      "  :\t:\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 18)\t2\n",
      "  (1, 27)\t2\n",
      "  (1, 28)\t1\n",
      "  (1, 29)\t1\n",
      "  (1, 32)\t1\n",
      "  (1, 33)\t1\n",
      "  (1, 34)\t1\n",
      "  (1, 35)\t2\n",
      "  (1, 38)\t4\n",
      "  (1, 40)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 43)\t1\n",
      "  (1, 44)\t1\n",
      "  (1, 47)\t1\n",
      "  (1, 49)\t7\n",
      "  (1, 50)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(ftr_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e05d47bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 38, 'matrix': 22, 'is': 19, 'everywhere': 11, 'its': 21, 'all': 0, 'around': 2, 'us': 41, 'here': 15, 'even': 10, 'in': 18, 'this': 39, 'room': 30, 'you': 49, 'can': 6, 'see': 31, 'it': 20, 'out': 25, 'your': 50, 'window': 46, 'or': 24, 'on': 23, 'television': 37, 'feel': 12, 'when': 45, 'go': 13, 'to': 40, 'work': 48, 'church': 7, 'pay': 26, 'taxes': 36, 'take': 35, 'blue': 5, 'pill': 27, 'and': 1, 'story': 34, 'ends': 9, 'wake': 42, 'bed': 3, 'believe': 4, 'whatever': 44, 'want': 43, 'red': 29, 'stay': 33, 'wonderland': 47, 'show': 32, 'how': 17, 'deep': 8, 'rabbit': 28, 'hole': 16, 'goes': 14}\n"
     ]
    }
   ],
   "source": [
    "print(cnt_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa135f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 5)\n",
      "{'the': 2, 'you': 3, 'your': 4, 'or': 1, 'and': 0}\n"
     ]
    }
   ],
   "source": [
    "cnt_vect = CountVectorizer(max_features=5)\n",
    "cnt_vect.fit(text)\n",
    "ftr_vect = cnt_vect.transform(text)\n",
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(cnt_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b856a4d0",
   "metadata": {},
   "source": [
    "#### ngram_range 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4a15ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 125)\n",
      "{'the': 82, 'matrix': 49, 'is': 42, 'everywhere': 25, 'its': 47, 'all': 0, 'around': 6, 'us': 94, 'here': 32, 'even': 23, 'in': 38, 'this': 88, 'room': 67, 'you': 110, 'can': 15, 'see': 69, 'it': 44, 'out': 57, 'your': 120, 'window': 104, 'or': 53, 'on': 51, 'television': 80, 'feel': 27, 'when': 102, 'go': 29, 'to': 90, 'work': 108, 'church': 17, 'pay': 59, 'taxes': 79, 'the matrix': 84, 'matrix is': 50, 'is everywhere': 43, 'everywhere its': 26, 'its all': 48, 'all around': 1, 'around us': 7, 'us here': 95, 'here even': 33, 'even in': 24, 'in this': 39, 'this room': 89, 'room you': 68, 'you can': 112, 'can see': 16, 'see it': 70, 'it out': 45, 'out your': 58, 'your window': 124, 'window or': 105, 'or on': 55, 'on your': 52, 'your television': 123, 'television you': 81, 'you feel': 113, 'feel it': 28, 'it when': 46, 'when you': 103, 'you go': 114, 'go to': 30, 'to work': 93, 'work or': 109, 'or go': 54, 'to church': 92, 'church or': 18, 'or pay': 56, 'pay your': 60, 'your taxes': 122, 'take': 77, 'blue': 13, 'pill': 61, 'and': 2, 'story': 75, 'ends': 21, 'wake': 96, 'bed': 8, 'believe': 10, 'whatever': 100, 'want': 98, 'red': 65, 'stay': 73, 'wonderland': 106, 'show': 71, 'how': 36, 'deep': 19, 'rabbit': 63, 'hole': 34, 'goes': 31, 'you take': 117, 'take the': 78, 'the blue': 83, 'blue pill': 14, 'pill and': 62, 'and the': 4, 'the story': 87, 'story ends': 76, 'ends you': 22, 'you wake': 118, 'wake in': 97, 'in your': 41, 'your bed': 121, 'bed and': 9, 'and you': 5, 'you believe': 111, 'believe whatever': 11, 'whatever you': 101, 'you want': 119, 'want to': 99, 'to believe': 91, 'believe you': 12, 'the red': 86, 'red pill': 66, 'you stay': 116, 'stay in': 74, 'in wonderland': 40, 'wonderland and': 107, 'and show': 3, 'show you': 72, 'you how': 115, 'how deep': 37, 'deep the': 20, 'the rabbit': 85, 'rabbit hole': 64, 'hole goes': 35}\n"
     ]
    }
   ],
   "source": [
    "# cnt_vect = CountVectorizer(ngram_range=(1,2)) # unigram\n",
    "cnt_vect = CountVectorizer(ngram_range=(1,2)) # unigram + bigram\n",
    "# cnt_vect = CountVectorizer(ngram_range=(1,3)) # unigram + bigram + trigram\n",
    "cnt_vect.fit(text)\n",
    "ftr_vect = cnt_vect.transform(text)\n",
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(cnt_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84b8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
