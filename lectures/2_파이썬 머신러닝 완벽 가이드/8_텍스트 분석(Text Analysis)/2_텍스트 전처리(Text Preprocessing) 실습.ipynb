{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b651cf18",
   "metadata": {},
   "source": [
    "## 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00fa1d9",
   "metadata": {},
   "source": [
    "### Text Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d593bd2a",
   "metadata": {},
   "source": [
    "### 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92949121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aaaaaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## conda install -c anaconda nltk\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75305953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "text_sample = \"\"\"The Matrix is everywhere its all around us, here even in this room.\n",
    "You can see it out your window or on your television.\n",
    "You feel it when you go to work, or go to church or pay your taxes.\"\"\"\n",
    "\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95787a2",
   "metadata": {},
   "source": [
    "### 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2577387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624c5f1",
   "metadata": {},
   "source": [
    "### 여러 문장들에 대한 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a92fde54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# 여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# 여러 문장들에 대해 문장별 단어 토큰화 수행.\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf9c816",
   "metadata": {},
   "source": [
    "### n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b842ba2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'Matrix', 'is'), ('Matrix', 'is', 'everywhere'), ('is', 'everywhere', 'its'), ('everywhere', 'its', 'all'), ('its', 'all', 'around'), ('all', 'around', 'us'), ('around', 'us', ','), ('us', ',', 'here'), (',', 'here', 'even'), ('here', 'even', 'in'), ('even', 'in', 'this'), ('in', 'this', 'room'), ('this', 'room', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "word = word_tokenize(sentence)\n",
    "\n",
    "all_ngrams = ngrams(words, 3)\n",
    "ngrams = [ngram for ngram in all_ngrams]\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc8d95f",
   "metadata": {},
   "source": [
    "### Stopwords 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52b51ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aaaaaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aaaaaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0241f2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수 :  179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop words 개수 : ', len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c6d57bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "# 위 예제의 3개의 문장별로 얻은 word_tokens list에 대해 stop word 제거 Loop\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거 Loop\n",
    "    for word in sentence:\n",
    "        # 소문자로 모두 변환합니다.\n",
    "        word = word.lower()\n",
    "        # tokenize된 개별 word가 stop words들의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64b06f",
   "metadata": {},
   "source": [
    "### Stemming과 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dc39812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49f4f793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
    "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
    "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146221f",
   "metadata": {},
   "source": [
    "### 텍스트의 피처 벡터화 유형\n",
    "\n",
    "1. BOW(Bag of words)\n",
    "    * Document Term Matrix : 개별 문서(또는 문장)을 단어들의 횟수나 정규화 변환된 횟수로 표현\n",
    "2. Word Embedding(Word2Vec)\n",
    "    * 개별 단어를 문맥을 가지는 N차원 공간에 벡터로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c10f701",
   "metadata": {},
   "source": [
    "### Bag of Words - BOW\n",
    "\n",
    "* Bag of Words 모델은 문서가 가지는 모든 단어(Words)를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델입니다. 문서 내 모든 단어를 한꺼번에 봉투(Bag)안에 넣은 뒤에 흔들어서 섞는다는 의미로 Bag of Words(BOW) 모델이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a6108",
   "metadata": {},
   "source": [
    "### Bag of Words 구조\n",
    "\n",
    "* 문장 1: My wife likes to watch baseball games and my daughter likes to watch baseball games too\n",
    "* 문장 2: My wife likes to play baseball\n",
    "\n",
    "1. 문장 1과 문장 2에 있는 모든 단어에서 중복을 제거하고 각 단어(feature 또는 term)를 컬럼 형태로 나열합니다. 그러고 나서 각 단어에 고유의 인덱스를 다음과 같이 부여합니다. 'and':0, 'baseball':1, 'daughter':2 ...\n",
    "2. 문장에서 해당 단어가 나타나는 횟수(Occurrence)를 각 단어(단어 인덱스)에 기재합니다. 예를 들어 baseball은 문장 1,2에서 총 2번 나타나며, daughter는 문장1에서만 1번 나타납니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e630f035",
   "metadata": {},
   "source": [
    "### BOW 장단점\n",
    "\n",
    "* 장점\n",
    "    * 쉽고 빠른 구축\n",
    "    * \"예상보다\" 문서의 특징을 잘 나타내어 전통적으로 여러분야에서 활용도가 높음\n",
    "* 단점\n",
    "    * 문맥 의미(semantic context) 반영 문제\n",
    "    * 희소 행렬 문제\n",
    "        * document term matrix가 가지는 고질적인 문제(값이 없는...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b54fdd",
   "metadata": {},
   "source": [
    "### BOW 피처 벡터화(Feature Vectorization)\n",
    "\n",
    "* M개의 Text문서들 -> M * N 피처 벡터화(m개의 문서, n개의 피처)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa73994e",
   "metadata": {},
   "source": [
    "### BOW 피처 벡터화 유형\n",
    "\n",
    "* 단순 카운트 기반의 벡터화\n",
    "    * 단어 피처에 값을 부여할 때, 각 문서에서 해당 단어가 나타나는 횟수, 즉 Count를 부여하는 경우를 카운트 벡터화라고 합니다. 카운트 벡터화에서는 카운트 값이 높을수록 중요한 단어로 인식됩니다.\n",
    "* TF-IDF 벡터화\n",
    "    * 카운트만 부여할 경우, 그 문서의 특징을 나타내기보다는 언어의 특성상 문장에서 자주 사용될 수밖에 없는 단어까지 높은 값을 부여하게 됩니다. 이러한 문제를 보완하기 위해 TF-IDF(Term Frequency Inverse Document Frequency) 벡터화를 사용합니다.\n",
    "    * TF-IDF는 **개별 문서에서 자주 나타나는 단어에 높은 가중치**를 주되, **모든 문서에서 전반적으로 자주 나타나는 단어에 대해서는 패널티**를 주는 방식으로 값을 부여합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a726e",
   "metadata": {},
   "source": [
    "### TF-IDF(Term Frequency Inverse Document Frequency)\n",
    "\n",
    "* 특정 단어가 다른 문서에서 나타나지 않고, 특정 문서에서만 자주 사용된다면, 해당 단어는 해당 문서를 잘 특징짓는 중요 단어일 가능성이 높음\n",
    "* 특정 단어가 매우 많은 여러 문서에서 빈번히 나타난다면 해당 단어는 개별 문서를 특정짓는 정보로서의 의미를 상실\n",
    "\n",
    "1. TF(Term Frequency) : 문서에서 해당 단어가 얼마나 나왔는지를 나타내는 지표\n",
    "2. DF(Document Frequency) : 해당 단어가 몇개의 문서에서 나타났는지를 나타내는 좌표\n",
    "3. IDF(Inverse Docuent Frequency) : DF의 역수로서 전체 문서수/DF\n",
    "\n",
    "* $TFIDF_i = TF_i * log \\frac{N}{DF_i}$\n",
    "    * $TF_i$ = 개별 단어에서의 단어 i의 빈도\n",
    "    * $DF_i$ = 단어 i를 가지고 있는 문서 개수\n",
    "    * N = 전체 문서 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c31c167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
